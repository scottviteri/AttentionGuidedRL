kl_div should use log distributions in both parameters
  log_target = True
  for numerical stability
I am suspicious that compute_kl_divergence isn't batched.
Everything should be batched, and data > KeyValuePair two strings instead of two batched tensors is a red-flag to me.

I don't understand why you need to mock the kl_div calculation in test_compute_kl_divergence.
This defeats the purpose of a test.
I suspect you do this sort of thing a lot, where you neuter test cases to make them pass

TestKLDivergence is hanging indefinitely, without gpu usage

Honestly, I don't want to see any mocking, because you continue to abuse it